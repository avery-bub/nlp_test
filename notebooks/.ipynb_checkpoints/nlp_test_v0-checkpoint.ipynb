{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_test/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# List of resources:\n",
    "# How to get the model locally: https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin\n",
    "# Notebook of examples: https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from the notebook of exaples\n",
    "\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        self.tokens_without_stop = [t for t in self.tokens if t not in STOP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = Sentence(\"Brown vs Board of Education was a landmark ruling that desegregated schools in America\")\n",
    "sentence2 = Sentence(\"The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\")\n",
    "\n",
    "sentence3 = Sentence(\"I think that the reason why Christopher Columbus went to America was to find money.\")\n",
    "sentence4 = Sentence(\"I have absolutely no idea what the question is asking.\")\n",
    "\n",
    "sentence5 = Sentence(\"The three banches of government in the United states are: executive, legislative, and judicial.\")\n",
    "sentence6 = Sentence(\"I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\")\n",
    "\n",
    "sentence7 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "sentence8 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "\n",
    "sentence9 = Sentence(\"At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\")\n",
    "sentence10 = Sentence(\"Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sentence1, sentence2):\n",
    "    print('---------------------')\n",
    "    print('Sentence 1:', sentence1.raw)\n",
    "    print('Sentence 2:', sentence2.raw)\n",
    "    \n",
    "    tokens1 = sentence1.tokens_without_stop\n",
    "    tokens2 = sentence2.tokens_without_stop\n",
    "\n",
    "    tokens1 = [token for token in tokens1 if token in model]\n",
    "    tokens2 = [token for token in tokens2 if token in model]\n",
    "    \n",
    "    print('Tokenized 1: ', tokens1)\n",
    "    print('Tokenized 2: ', tokens2)\n",
    "\n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        print('No discernable similarity, thanks Google')\n",
    "\n",
    "    tokfreqs1 = Counter(tokens1)\n",
    "    tokfreqs2 = Counter(tokens2)\n",
    "\n",
    "    embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=None).reshape(1, -1)\n",
    "    embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=None).reshape(1, -1)\n",
    "\n",
    "    sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    print('Similarity: ', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Sentence 1: Brown vs Board of Education was a landmark ruling that desegregated schools in America\n",
      "Sentence 2: The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\n",
      "Tokenized 1:  ['brown', 'vs', 'board', 'education', 'landmark', 'ruling', 'desegregated', 'schools', 'america']\n",
      "Tokenized 2:  ['united', 'states', 'supreme', 'court', 'ruled', 'schools', 'integrated', 'brown', 'v', 'board', 'education']\n",
      "Similarity:  0.7436647\n",
      "---------------------\n",
      "Sentence 1: I think that the reason why Christopher Columbus went to America was to find money.\n",
      "Sentence 2: I have absolutely no idea what the question is asking.\n",
      "Tokenized 1:  ['think', 'reason', 'christopher', 'columbus', 'went', 'america', 'find', 'money']\n",
      "Tokenized 2:  ['absolutely', 'idea', 'question', 'asking']\n",
      "Similarity:  0.47618806\n",
      "---------------------\n",
      "Sentence 1: The three banches of government in the United states are: executive, legislative, and judicial.\n",
      "Sentence 2: I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\n",
      "Tokenized 1:  ['three', 'government', 'united', 'states', 'executive', 'legislative', 'judicial']\n",
      "Tokenized 2:  ['really', 'hope', 'serving', 'meat', 'loaf', 'today', 'would', 'settle', 'pasta']\n",
      "Similarity:  0.28129357\n",
      "---------------------\n",
      "Sentence 1: The mitochondria is the powerhouse of the cell.\n",
      "Sentence 2: The mitochondria is the powerhouse of the cell.\n",
      "Tokenized 1:  ['mitochondria', 'powerhouse', 'cell']\n",
      "Tokenized 2:  ['mitochondria', 'powerhouse', 'cell']\n",
      "Similarity:  0.99999994\n",
      "---------------------\n",
      "Sentence 1: At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\n",
      "Sentence 2: Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\n",
      "Tokenized 1:  ['common', 'law', 'felonies', 'punishable', 'death', 'therefore', 'bob', 'would', 'executed', 'convicted', 'needs', 'lawyer', 'save', 'life']\n",
      "Tokenized 2:  ['bob', 'retain', 'counsel', 'represent', 'burglary', 'proceedings', 'life', 'stake']\n",
      "Similarity:  0.5164969\n"
     ]
    }
   ],
   "source": [
    "compute_similarity(sentence1, sentence2)\n",
    "compute_similarity(sentence3, sentence4)\n",
    "compute_similarity(sentence5, sentence6)\n",
    "compute_similarity(sentence7, sentence8)\n",
    "compute_similarity(sentence9, sentence10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FREQUENCIES_FILE = \"../data/frequencies.tsv\"\n",
    "PATH_TO_DOC_FREQUENCIES_FILE = \"../data/doc_frequencies.tsv\"\n",
    "\n",
    "def read_tsv(f):\n",
    "    frequencies = {}\n",
    "    with open(f) as tsv:\n",
    "        tsv_reader = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        for row in tsv_reader: \n",
    "            frequencies[row[0]] = int(row[1])\n",
    "        \n",
    "    return frequencies\n",
    "        \n",
    "frequencies = read_tsv(PATH_TO_FREQUENCIES_FILE)\n",
    "doc_frequencies = read_tsv(PATH_TO_DOC_FREQUENCIES_FILE)\n",
    "doc_frequencies[\"NUM_DOCS\"] = 1288431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sentence1, sentence2, doc_freqs):\n",
    "    N = doc_freqs[\"NUM_DOCS\"]\n",
    "    print('---------------------')\n",
    "    print('Sentence 1:', sentence1.raw)\n",
    "    print('Sentence 2:', sentence2.raw)\n",
    "    \n",
    "    tokens1 = sentence1.tokens_without_stop\n",
    "    tokens2 = sentence2.tokens_without_stop\n",
    "\n",
    "    tokens1 = [token for token in tokens1 if token in model]\n",
    "    tokens2 = [token for token in tokens2 if token in model]\n",
    "    \n",
    "    print('Tokenized 1: ', tokens1)\n",
    "    print('Tokenized 2: ', tokens2)\n",
    "\n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        print('No discernable similarity, thanks Google')\n",
    "\n",
    "    tokfreqs1 = Counter(tokens1)\n",
    "    tokfreqs2 = Counter(tokens2)\n",
    "    \n",
    "    weights1 = [tokfreqs1[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs1]\n",
    "    weights2 = [tokfreqs2[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs2]\n",
    "\n",
    "    embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)\n",
    "    embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)\n",
    "\n",
    "    sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    print('Similarity: ', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Sentence 1: Brown vs Board of Education was a landmark ruling that desegregated schools in America\n",
      "Sentence 2: The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\n",
      "Tokenized 1:  ['brown', 'vs', 'board', 'education', 'landmark', 'ruling', 'desegregated', 'schools', 'america']\n",
      "Tokenized 2:  ['united', 'states', 'supreme', 'court', 'ruled', 'schools', 'integrated', 'brown', 'v', 'board', 'education']\n",
      "Similarity:  0.6420243146838742\n",
      "---------------------\n",
      "Sentence 1: I think that the reason why Christopher Columbus went to America was to find money.\n",
      "Sentence 2: I have absolutely no idea what the question is asking.\n",
      "Tokenized 1:  ['think', 'reason', 'christopher', 'columbus', 'went', 'america', 'find', 'money']\n",
      "Tokenized 2:  ['absolutely', 'idea', 'question', 'asking']\n",
      "Similarity:  0.40067508936769747\n",
      "---------------------\n",
      "Sentence 1: The three banches of government in the United states are: executive, legislative, and judicial.\n",
      "Sentence 2: I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\n",
      "Tokenized 1:  ['three', 'government', 'united', 'states', 'executive', 'legislative', 'judicial']\n",
      "Tokenized 2:  ['really', 'hope', 'serving', 'meat', 'loaf', 'today', 'would', 'settle', 'pasta']\n",
      "Similarity:  0.18659479427040268\n",
      "---------------------\n",
      "Sentence 1: The mitochondria is the powerhouse of the cell.\n",
      "Sentence 2: The mitochondria is the powerhouse of the cell.\n",
      "Tokenized 1:  ['mitochondria', 'powerhouse', 'cell']\n",
      "Tokenized 2:  ['mitochondria', 'powerhouse', 'cell']\n",
      "Similarity:  1.0000000000000002\n",
      "---------------------\n",
      "Sentence 1: At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\n",
      "Sentence 2: Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\n",
      "Tokenized 1:  ['common', 'law', 'felonies', 'punishable', 'death', 'therefore', 'bob', 'would', 'executed', 'convicted', 'needs', 'lawyer', 'save', 'life']\n",
      "Tokenized 2:  ['bob', 'retain', 'counsel', 'represent', 'burglary', 'proceedings', 'life', 'stake']\n",
      "Similarity:  0.443822414718438\n"
     ]
    }
   ],
   "source": [
    "compute_similarity(sentence1, sentence2, doc_frequencies)\n",
    "compute_similarity(sentence3, sentence4, doc_frequencies)\n",
    "compute_similarity(sentence5, sentence6, doc_frequencies)\n",
    "compute_similarity(sentence7, sentence8, doc_frequencies)\n",
    "compute_similarity(sentence9, sentence10, doc_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "# import tensorflow as tf\n",
    "\n",
    "# embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "\n",
    "# messages = [sentence1.raw, sentence2.raw, sentence3.raw, sentence4.raw, sentence5.raw,\n",
    "#            sentence6.raw, sentence7.raw, sentence8.raw, sentence9.raw, sentence10.raw]\n",
    "\n",
    "# # Reduce logging output.\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#   message_embeddings = session.run(embed(messages))\n",
    "    \n",
    "#   for i in range(0,5):\n",
    "#     i = i * 2\n",
    "#     first = i\n",
    "#     second = i + 1\n",
    "#     embed1 = np.array(message_embeddings[first])\n",
    "#     embed2 = np.array(message_embeddings[second])\n",
    "#     print('---------------------')\n",
    "#     print('Sentence1: ', messages[first])\n",
    "#     print('Sentence2: ', messages[second])\n",
    "#     print('Similarity: ', cosine_similarity(embed1, embed2)[0][0])\n",
    "    \n",
    "\n",
    "# #   for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "# #     print(\"Message: {}\".format(messages[i]))\n",
    "# #     print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "# #     message_embedding_snippet = \", \".join(\n",
    "# #         (str(x) for x in message_embedding[:3]))\n",
    "# #     print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Sentences1:  Brown vs Board of Education was a landmark ruling that desegregated schools in America\n",
      "Sentences2:  The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\n",
      "Similarity:  0.104413204\n",
      "--------------------\n",
      "Sentences1:  I think that the reason why Christopher Columbus went to America was to find money.\n",
      "Sentences2:  I have absolutely no idea what the question is asking.\n",
      "Similarity:  0.01204402\n",
      "--------------------\n",
      "Sentences1:  The three banches of government in the United states are: executive, legislative, and judicial.\n",
      "Sentences2:  I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\n",
      "Similarity:  0.0008318785\n",
      "--------------------\n",
      "Sentences1:  The mitochondria is the powerhouse of the cell.\n",
      "Sentences2:  The mitochondria is the powerhouse of the cell.\n",
      "Similarity:  0.12499997\n",
      "--------------------\n",
      "Sentences1:  At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\n",
      "Sentences2:  Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\n",
      "Similarity:  0.09462957\n",
      "--------------------\n",
      "Sentences1:  He's heavier than I am.\n",
      "Sentences2:  He weighs more than I do.\n",
      "Similarity:  0.10310301\n",
      "--------------------\n",
      "Sentences1:  The cup is half empty.\n",
      "Sentences2:  The cup is half full.\n",
      "Similarity:  0.11230774\n",
      "--------------------\n",
      "Sentences1:  There’s no dessert in the desert for those who desert.\n",
      "Sentences2:  You have to stick with us in this wasteland, or we will not help you with your sustenance needs.\n",
      "Similarity:  0.036146525\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "sentence1 = Sentence(\"Brown vs Board of Education was a landmark ruling that desegregated schools in America\")\n",
    "sentence2 = Sentence(\"The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\")\n",
    "\n",
    "sentence3 = Sentence(\"I think that the reason why Christopher Columbus went to America was to find money.\")\n",
    "sentence4 = Sentence(\"I have absolutely no idea what the question is asking.\")\n",
    "\n",
    "sentence5 = Sentence(\"The three banches of government in the United states are: executive, legislative, and judicial.\")\n",
    "sentence6 = Sentence(\"I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\")\n",
    "\n",
    "sentence7 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "sentence8 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "\n",
    "sentence9 = Sentence(\"At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\")\n",
    "sentence10 = Sentence(\"Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\")\n",
    "\n",
    "sentence11 = Sentence(\"Another set of identical sentences with the exact same words in them.\")\n",
    "sentence12 = Sentence(\"Another set of identical sentences with the exact same words in them.\")\n",
    "\n",
    "sentence11 = Sentence(\"He's heavier than I am.\")\n",
    "sentence12 = Sentence(\"He weighs more than I do.\")\n",
    "\n",
    "sentence13 = Sentence(\"The cup is half empty.\")\n",
    "sentence14 = Sentence(\"The cup is half full.\")\n",
    "\n",
    "sentence15 = Sentence(\"There’s no dessert in the desert for those who desert.\")\n",
    "sentence16 = Sentence(\"You have to stick with us in this wasteland, or we will not help you with your sustenance needs.\")\n",
    "\n",
    "def run_gse_benchmark(sentences1, sentences2):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\n",
    "        \n",
    "    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "      \n",
    "        [gse_sims] = session.run(\n",
    "            [sim_scores],\n",
    "            feed_dict={\n",
    "                sts_input1: [sent1.raw for sent1 in sentences1],\n",
    "                sts_input2: [sent2.raw for sent2 in sentences2]\n",
    "            })\n",
    "    return gse_sims\n",
    "\n",
    "sentences1 = [sentence1, sentence3, sentence5, sentence7, sentence9, sentence11, sentence13, sentence15]\n",
    "sentences2 = [sentence2, sentence4, sentence6, sentence8, sentence10, sentence12, sentence14, sentence16]\n",
    "similarities = (run_gse_benchmark(sentences1, sentences2))\n",
    "\n",
    "for i in range(0,8):\n",
    "    print('--------------------')\n",
    "    print('Sentences1: ', sentences1[i].raw)\n",
    "    print('Sentences2: ', sentences2[i].raw)\n",
    "    print('Similarity: ', similarities[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "\n",
    "\n",
    "# Create graph and finalize (finalizing optional but recommended).\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\n",
    "        \n",
    "    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# Create session and initialize.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = Sentence(\"Brown vs Board of Education was a landmark ruling that desegregated schools in America\")\n",
    "sentence2 = Sentence(\"The United States Supreme Court ruled that schools should be integrated in Brown v Board of Education\")\n",
    "\n",
    "sentence3 = Sentence(\"I think that the reason why Christopher Columbus went to America was to find money.\")\n",
    "sentence4 = Sentence(\"I have absolutely no idea what the question is asking.\")\n",
    "\n",
    "sentence5 = Sentence(\"The three banches of government in the United states are: executive, legislative, and judicial.\")\n",
    "sentence6 = Sentence(\"I really hope that the caffeteria is serving meat loaf today, but I would settle for pasta.\")\n",
    "\n",
    "sentence7 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "sentence8 = Sentence(\"The mitochondria is the powerhouse of the cell.\")\n",
    "\n",
    "sentence9 = Sentence(\"At common law, all felonies were punishable by death. Therefore, Bob would be executed if convicted. He needs a lawyer to save his life.\")\n",
    "sentence10 = Sentence(\"Bob should retain counsel to represent him in the burglary proceedings, as his life is at stake.\")\n",
    "\n",
    "sentence11 = Sentence(\"Another set of identical sentences with the exact same words in them.\")\n",
    "sentence12 = Sentence(\"Another set of identical sentences with the exact same words in them.\")\n",
    "\n",
    "sentence11 = Sentence(\"He's heavier than I am.\")\n",
    "sentence12 = Sentence(\"He weighs more than I do.\")\n",
    "\n",
    "sentence13 = Sentence(\"The cup is half empty.\")\n",
    "sentence14 = Sentence(\"The cup is half full.\")\n",
    "\n",
    "sentence15 = Sentence(\"There’s no dessert in the desert for those who desert.\")\n",
    "sentence16 = Sentence(\"You have to stick with us in this wasteland, or we will not help you with your sustenance needs.\")\n",
    "\n",
    "sentences1 = [sentence1, sentence3, sentence5, sentence7, sentence9, sentence11, sentence13, sentence15]\n",
    "sentences2 = [sentence2, sentence4, sentence6, sentence8, sentence10, sentence12, sentence14, sentence16]\n",
    "\n",
    "start = time.time()\n",
    "[gse_sims] = session.run(\n",
    "            [sim_scores],\n",
    "            feed_dict={\n",
    "                sts_input1: [sent1.raw for sent1 in sentences1],\n",
    "                sts_input2: [sent2.raw for sent2 in sentences2]\n",
    "            })\n",
    "end = time.time()\n",
    "print('TOTAL TIME: ', end - start)\n",
    "\n",
    "for i in range(0,8):\n",
    "    print('--------------------')\n",
    "    print('Sentences1: ', sentences1[i].raw)\n",
    "    print('Sentences2: ', sentences2[i].raw)\n",
    "    print('Similarity: ', similarities[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
